--- 
title: "Variational Inference: Interpreted and Visualized"
author: "Austin Tao"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

# Project Proposal

## Background/Motivation

Bayesian statistics is an ever-growing field in modern statistics. Inference from a Bayesian lens frames all problems about unknown quantities as a calculation involving the posterior density. However, it is often difficult to actually analytically derive these probability density functions (PDF). Instead, we often turn to approximating them using methods such as Markov Chain Monte Carlo (MCMC) sampling.

However, this is not the only way to approximate such PDFs. One alternative method is called Variational Inference (VI), which is a machine learning method that performs these approximations using optimization. VI does this by positing an entire family of potential density functions, then optimizes by finding the member of that family that most closely matches the data.

To actually perform VI is no trivial task, especially so because many of the calculations required are intractable. So, to accomplish the optimization task that VI poses, we need to use an iterative algorithm that performs the optimization. One such algorithm is called the Coordinate Ascent Variational Inference (CAVI) algorithm. 


## Modeling Goal/Data

The goal for this project is to demonstrate the usage of VI and comment on its interpretability. The primary example used here will be applying CAVI to a linear regression task and comparing how interpretable the steps are using CAVI as opposed to standard OLS optimization. Specifically, this model will be trained and tested on the [Facebook Metrics Dataset](https://archive.ics.uci.edu/ml/datasets/Facebook+metrics), where the specific regression task will be to predict the number of "Lifetime Engaged Users" a particular post will receive.


## Existing Literature

The task of applying VI to linear regression has already been done in the past (see [here](https://fabiandablander.com/r/Variational-Inference.html)). However, the existing literature makes no comment on the interpretability of the models and only does limited visualization. Additionally, the evaluation and training of this model was done on artificially generated data rather than real-world data. This project will address both of these points and contribute to the existing literature.

## Further Extension

After comparing the interpretability and visualizability of linear regression when optimized using VI versus OLS, I would also like to to compare VI, which is a parametric modeling method, to a nonparametric modeling method. Specifically, I am hoping to compare with a Gaussian Processes Regression (GPR) model, building the model based off of [this](https://arxiv.org/pdf/2009.10862.pdf) paper. I hope to familiarize myself with this particular nonparametric model, building off of my understanding of the parametric VI model.



