[["index.html", "Variational Inference: Interpreted and Visualized Chapter 1 Project Proposal 1.1 Background/Motivation 1.2 Modeling Goal/Data 1.3 Existing Literature 1.4 Further Extension", " Variational Inference: Interpreted and Visualized Austin Tao 2023-04-13 Chapter 1 Project Proposal 1.1 Background/Motivation Bayesian statistics is an ever-growing field in modern statistics. Inference from a Bayesian lens frames all problems about unknown quantities as a calculation involving the posterior density. However, it is often difficult to actually analytically derive these probability density functions (PDF). Instead, we often turn to approximating them using methods such as Markov Chain Monte Carlo (MCMC) sampling. However, this is not the only way to approximate such PDFs. One alternative method is called Variational Inference (VI), which is a machine learning method that performs these approximations using optimization. VI does this by positing an entire family of potential density functions, then optimizes by finding the member of that family that most closely matches the data. To actually perform VI is no trivial task, especially so because many of the calculations required are intractable. So, to accomplish the optimization task that VI poses, we need to use an iterative algorithm that performs the optimization. One such algorithm is called the Coordinate Ascent Variational Inference (CAVI) algorithm. 1.2 Modeling Goal/Data The goal for this project is to demonstrate the usage of VI and comment on its interpretability. The primary example used here will be applying CAVI to a linear regression task and comparing how interpretable the steps are using CAVI as opposed to standard OLS optimization. Specifically, this model will be trained and tested on the Facebook Metrics Dataset, where the specific regression task will be to predict the number of “Lifetime Engaged Users” a particular post will receive. 1.3 Existing Literature The task of applying VI to linear regression has already been done in the past (see here). However, the existing literature makes no comment on the interpretability of the models and only does limited visualization. Additionally, the evaluation and training of this model was done on artificially generated data rather than real-world data. This project will address both of these points and contribute to the existing literature. 1.4 Further Extension After comparing the interpretability and visualizability of linear regression when optimized using VI versus OLS, I would also like to to compare VI, which is a parametric modeling method, to a nonparametric modeling method. Specifically, I am hoping to compare with a Gaussian Processes Regression (GPR) model, building the model based off of this paper. I hope to familiarize myself with this particular nonparametric model, building off of my understanding of the parametric VI model. "],["introduction-to-variational-inference.html", "Chapter 2 Introduction to Variational Inference 2.1 Setting Up the Problem 2.2 The Mean-Field Variational Family 2.3 Coordinate Ascent Variational Inference", " Chapter 2 Introduction to Variational Inference In the existing statistics and machine learning literature, there are many methods for approximating probability densities. This is especially important for Bayesian inference, which centers around finding good approximations for the posterior distribution. Variational Inference (VI) is one such technique that can be used. 2.1 Setting Up the Problem We want to approximate the posterior distribution given some observed variables \\(\\textbf{X}\\) and a set of latent variables \\(\\textbf{z}\\). If we have the joint probability density function \\(p(\\textbf{z}, \\textbf{X})\\), we can write the formula for the posterior as \\[\\begin{equation} p(\\textbf{z} \\mid \\textbf{X}) = \\frac{p(\\textbf{z}, \\textbf{X})}{p(\\textbf{X})} \\end{equation}\\] Though we have this explicit formula, we are actually unable to calculate the denominator (also called the evidence). So, rather than solving explicitly, we consider an entire family of probability densities \\(\\mathcal{D}\\) and find a density \\(q^*(\\textbf{z}) \\in \\mathcal{D}\\) that best approximates the posterior. More formally, we wish to find \\[\\begin{equation} q^*(\\textbf{z}) = \\mathop{\\mathrm{argmin}}\\limits_{q(\\textbf{z}) \\in \\mathcal{D}} \\mathrm{KL}(q(\\textbf{z}) || p(\\textbf{z} \\mid \\textbf{X}))) \\end{equation}\\] where \\(\\mathrm{KL}\\) is the Kullback-Libler Divergence. However, it turns out that computing the KL-divergence requires us to know the evidence, which we have already stated to be intractable. So, rather than optimizing the KL-divergence, we optimize a different objective that is equivalent up to an added constant, called the Evidence Lower Bound (ELBO). It is defined as follows: \\[\\begin{equation} \\mathrm{ELBO}(q) := \\mathbb{E}[\\log p(\\textbf{z}, \\textbf{X})] - \\mathbb{E}[\\log q(\\textbf{z})] \\end{equation}\\] Relating it to the KL-divergence, we have \\[\\begin{equation} \\mathrm{ELBO}(q) = \\mathbb{E}[\\log p(\\textbf{X} \\mid \\textbf{z})] - \\mathrm{KL}(q(\\textbf{z}) || p(\\textbf{z})) \\end{equation}\\] 2.2 The Mean-Field Variational Family Now, we have set up our optimization problem, but we have yet to actually specify the family of functions that our approximation is drawn from. One of the frequently used families is the Mean-Field Variational Family, which makes the assumption that the latent variables are mutually independent, each governed by their own variational density. Each member of this family of distributions has the form \\[\\begin{equation} q(\\textbf{z}) = \\prod_{j = 1}^m q_j(z_j) \\end{equation}\\] Here, the densities \\(q_j(z_j)\\) are the variational factors, and these are the specific factors that we will vary to maximize the ELBO. 2.3 Coordinate Ascent Variational Inference Between the setup of the ELBO and the mean-field family, we finally have a complete optimization problem to solve. Now, in order to solve this optimization problem, we choose to use Coordinate Ascent Variational Inference (CAVI), which iteratively optimizes each mean-field variational density and finds a local optimum for the ELBO. The pseudocode for CAVI is presented below: "],["linear-regression-with-variational-inference.html", "Chapter 3 Linear Regression with Variational Inference 3.1 Linear Regression Setup", " Chapter 3 Linear Regression with Variational Inference 3.1 Linear Regression Setup A standard ordinary least squares linear regression model has the form \\[\\begin{equation} y = \\beta_0 + \\sum_{i = 1}^p \\beta_ix_i + \\varepsilon \\end{equation}\\] where we assume there are \\(p\\) number of explanatory variables. For us, rather than trying to explicitly find the coefficients of the explanatory variables, we want to use CAVI to do so. So, we need to assume families of distributions that our latent variable, \\(y\\), and our model variables \\(\\beta\\) are drawn from. Additionally, we add a variance term \\(\\sigma^2\\). The setup is as follows: \\[\\begin{align} y &amp;\\sim \\mathcal{N}(\\beta x, \\sigma^2) &amp; \\beta &amp;\\sim \\mathcal{N}(0, \\sigma^2\\tau^2) \\end{align}\\] "],["interpretability-of-linear-regression-optimized-with-variational-inference.html", "Chapter 4 Interpretability of Linear Regression Optimized with Variational Inference", " Chapter 4 Interpretability of Linear Regression Optimized with Variational Inference "],["reflections.html", "Chapter 5 Reflections", " Chapter 5 Reflections "]]
